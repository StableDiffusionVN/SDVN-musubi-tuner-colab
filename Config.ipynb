{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic2arg(config:dict):\n",
    "  arg = ''\n",
    "  for value in config:\n",
    "    arg += f'{value if str(config[value]) != \"False\" else \"\"} {\"\" if type(config[value]) == bool else config[value]} '\n",
    "  return arg\n",
    "\n",
    "#wan\n",
    "model_train_list = {\n",
    "  \"wan_t2v_13\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors\",\n",
    "  \"wan_t2v\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_t2v_14B_fp8_e4m3fn.safetensors\",\n",
    "  \"wan_i2v\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp8_e4m3fn.safetensors\",\n",
    "  \"vae_wan\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/vae/wan_2.1_vae.safetensors\",\n",
    "  \"tt5_clip\": \"https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/blob/main/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "  \"wan_clip_vision\": \"https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth\",\n",
    "  \"hun_t2v\": \"https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt\",\n",
    "  \"hun_i2v\": \"https://huggingface.co/tencent/HunyuanVideo-I2V/blob/main/hunyuan-video-i2v-720p/transformers/mp_rank_00_model_states.pt\",\n",
    "  \"hun_clip_vision\": \"https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/blob/main/split_files/clip_vision/llava_llama3_vision.safetensors\",\n",
    "  \"hun_text_encoder1\": \"https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/blob/main/split_files/text_encoders/llava_llama3_fp16.safetensors\",\n",
    "  \"hun_text_encoder2\": \"https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/blob/main/split_files/text_encoders/clip_l.safetensors\",\n",
    "  \"vae_hun\": \"https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/vae/pytorch_model.pt\"\n",
    "}\n",
    "\n",
    "model_folder = os.path.join(root_dir,\"models\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8KQYvGDO7cg"
   },
   "outputs": [],
   "source": [
    "def dic2arg(config:dict):\n",
    "  arg = ''\n",
    "  for value in config:\n",
    "    arg += f'{value if str(config[value]) != \"False\" else \"\"} {\"\" if type(config[value]) == bool else config[value]} '\n",
    "  return arg\n",
    "\n",
    "def hug_down(link,path):\n",
    "  name = path.split('/')[-1]\n",
    "  folder = path.split(name)[0]\n",
    "  if \"blob\" in link:\n",
    "    link = link.replace(\"blob\",\"resolve\")\n",
    "  !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -s 16 -k 1M {link} -d {folder} -o {name}\n",
    "\n",
    "def model_path(model):\n",
    "  model_name = model\n",
    "  model_name = model_train_list[model].split('/')[-1]\n",
    "  model_path = os.path.join(model_folder,model_name)\n",
    "  if not os.path.exists(model_path):\n",
    "    hug_down(model_train_list[model],model_path)\n",
    "  return model_path\n",
    "\n",
    "op = {\n",
    "    '--mixed_precision': 'bf16',\n",
    "    '--num_cpu_threads_per_process': 1,\n",
    "}\n",
    "\n",
    "task_dic = {\n",
    "    \"wan_t2v_13\":\" t2v-1.3B\",\n",
    "    \"wan_t2v\": \"t2v-14B\",\n",
    "    \"wan_i2v\": \"i2v-14B\"}\n",
    "\n",
    "def data_config(resolution, extension, batch_size):\n",
    "  return {\n",
    "    \"general\": {\n",
    "        \"resolution\": resolution,\n",
    "        \"caption_extension\": extension,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"enable_bucket\": True,\n",
    "        \"bucket_no_upscale\": False\n",
    "    },\n",
    "    \"datasets\": []\n",
    "  }\n",
    "\n",
    "def config(model_train, optimizer_type, learning_rate, lr_scheduler, network_dim, network_alpha, max_train_epochs, save_every_n_epochs, save_last_n_epochs, save_every_n_steps, output_dir, output_name):\n",
    "  dataset_config_path = os.path.join(root_dir,\"dataset.toml\")\n",
    "  return {\n",
    "    \"hv_train_network.py\": True if \"hun\" in model_train else False,\n",
    "    \"wan_train_network.py\": True if \"wan\" in model_train else False,\n",
    "    \"--task\": task_dic[model_train] if \"wan\" in model_train else False,\n",
    "    \"--dit\": f'\"{model_path(model_train)}\"',\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--sdpa\": True,\n",
    "    \"--flash_attn\": False,\n",
    "    \"--flash3\": False,\n",
    "    \"--sage_attn\": False,\n",
    "    \"--xformers\": False,\n",
    "    \"--split_attn\": True,\n",
    "    \"--mixed_precision\": \"bf16\",\n",
    "    \"--fp8_base\": True,\n",
    "    \"--optimizer_type\": optimizer_type,\n",
    "    \"--optimizer_args\": False,\n",
    "    \"--learning_rate\": learning_rate,\n",
    "    \"--lr_scheduler\": lr_scheduler,\n",
    "    \"--gradient_checkpointing\": True,\n",
    "    \"--gradient_accumulation_steps\": False,\n",
    "    \"--max_data_loader_n_workers\": 2,\n",
    "    \"--persistent_data_loader_workers\": True,\n",
    "    \"--network_module\": \"networks.lora\" if \"hun\" in model_train else \"networks.lora_wan\",\n",
    "    \"--network_dim\": network_dim,\n",
    "    \"--network_alpha\": network_alpha,\n",
    "    \"--network_dropout\": False,\n",
    "    \"--network_weights\": False,\n",
    "    \"--timestep_sampling\": \"shift\",\n",
    "    \"--discrete_flow_shift\": 7.0 if \"hun\" in model_train else 3.0,\n",
    "    \"--max_train_steps\": False,\n",
    "    \"--max_train_epochs\": max_train_epochs,\n",
    "    \"--save_every_n_epochs\": save_every_n_epochs,\n",
    "    \"--save_last_n_epochs\": False if save_last_n_epochs <= 0 else save_last_n_epochs,\n",
    "    \"--save_every_n_steps\": False if save_every_n_steps <= 0 else save_every_n_steps,\n",
    "    \"--seed\": 42,\n",
    "    \"--output_dir\": f'\"{output_dir}/{output_name}_{model_train}\"',\n",
    "    \"--output_name\": f'\"{output_name}\"'\n",
    "}\n",
    "\n",
    "sample_prompt_path = os.path.join(root_dir,\"prompt.txt\")\n",
    "\n",
    "def extra(model_train, sample_every_n_steps, wandb_api_key, output_name, save_state, save_state_on_train_end, resume, author, description):\n",
    "  return {\n",
    "    \"--vae\": f'\"{model_path(\"vae_wan\")}\"' if \"wan\" in model_train else f'\"{model_path(\"vae_hun\")}\"',\n",
    "    \"--t5\": f'\"{model_path(\"tt5_clip\")}\"' if \"wan\" in model_train else False,\n",
    "    \"--text_encoder1\": f'\"{model_path(\"hun_text_encoder1\")}\"' if \"hun\" in model_train else False,\n",
    "    \"--text_encoder2\": f'\"{model_path(\"hun_text_encoder2\")}\"' if \"hun\" in model_train else False,\n",
    "    \"--clip\": f'\"{model_path(\"wan_clip_vision\")}\"' if \"wan\" in model_train else f'\"{model_path(\"hun_clip_vision\")}\"',\n",
    "    \"--sample_prompts\": f'\"{sample_prompt_path}\"',\n",
    "    \"--sample_every_n_epochs\": False,\n",
    "    \"--sample_every_n_steps\": sample_every_n_steps,\n",
    "    \"--sample_at_first\": True if sample_every_n_steps > 0 else False,\n",
    "    \"--log_with\": \"wandb\" if wandb_api_key != \"\" else False,\n",
    "    \"--wandb_api_key\": wandb_api_key if wandb_api_key != \"\" else False,\n",
    "    \"--wandb_run_name\": f'\"{model_train}_{output_name}\"',\n",
    "    \"--save_state\": save_state,\n",
    "    \"--save_state_on_train_end\": save_state_on_train_end,\n",
    "    \"--resume\": resume if resume != \"\" else False,\n",
    "    '--metadata_title': f'\"{output_name}\"',\n",
    "    '--metadata_author': f'\"{author}\"',\n",
    "    '--metadata_description': f'\"{description}\"',\n",
    "    '--metadata_license': False, # \"MIT\"\n",
    "    '--metadata_tags': False, # \"sdvn.me\"\n",
    "}\n",
    "\n",
    "def prompt(model_train, sample_prompt, folder_train, sample_image_i2v_path):\n",
    "  sample_image_i2v_path = os.path.join(repo_dir, \"example.png\") if sample_image_i2v_path == \"\" else sample_image_i2v_path\n",
    "  if \"i2v\" in model_train:\n",
    "    prompt = f\"\"\"{sample_prompt if sample_prompt != \"\" else random_sample(folder_train)} --w {sample_size.split(\",\")[0]} --h {sample_size.split(\",\")[1]} --f 25 --i {sample_image_i2v_path}\"\"\"\n",
    "  else:\n",
    "    prompt = f\"\"\"{sample_prompt if sample_prompt != \"\" else random_sample(folder_train)} --w {sample_size.split(\",\")[0]} --h {sample_size.split(\",\")[1]} --f 25\"\"\"\n",
    "  write_file(sample_prompt_path,prompt)\n",
    "\n",
    "def dataset_file(Image_train, Video_train, image_dir, data_config, video_dir, model_train, num_repeats, frame_extraction, target_frames, frame_stride, frame_sample, max_frames):\n",
    "  dataset = os.path.join(root_dir,\"dataset.toml\")\n",
    "  if Image_train:\n",
    "    for dir in check_sub_dir(image_dir):\n",
    "      data_config[\"datasets\"].append({\n",
    "            \"image_directory\": dir,\n",
    "            \"cache_directory\": os.path.join(root_dir,f\"cache_{model_train}\"),\n",
    "            \"num_repeats\": repeat_dir(dir,num_repeats),\n",
    "            })\n",
    "  if Video_train:\n",
    "    for dir in check_sub_dir(video_dir):\n",
    "      data_config[\"datasets\"].append({\n",
    "            \"video_directory\": dir,\n",
    "            \"cache_directory\": os.path.join(dir,f\"cache_{model_train}\"),\n",
    "            \"num_repeats\": repeat_dir(dir,num_repeats),\n",
    "            \"frame_extraction\": frame_extraction,\n",
    "            \"target_frames\": target_frames,\n",
    "            \"frame_stride\": frame_stride,\n",
    "            \"frame_sample\": frame_sample,\n",
    "            \"max_frames\": max_frames,\n",
    "        })\n",
    "\n",
    "  with open(dataset, \"w\") as file:\n",
    "      toml.dump(data_config, file)\n",
    "\n",
    "def cache_latents_config(model_train):\n",
    "  clip_vision_path = model_path(\"wan_clip_vision\") if \"wan\" in model_train else model_path(\"hun_clip_vision\")\n",
    "  return {\n",
    "    \"python\": True,\n",
    "    \"cache_latents.py\": True if \"hun\" in model_train else False,\n",
    "    \"wan_cache_latents.py\": True if \"wan\" in model_train else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--vae\": f'\"{model_path(\"vae_wan\")}\"' if \"wan\" in model_train else f'\"{model_path(\"vae_hun\")}\"',\n",
    "    \"--vae_chunk_size\": 32 if \"hun\" in model_train else False,\n",
    "    \"--vae_tiling\": True if \"hun\" in model_train else False,\n",
    "    \"--clip\": f'\"{clip_vision_path}\"' if \"i2v\" in model_train else False,\n",
    "}\n",
    "\n",
    "def cache_text_encoder_config(model_train):\n",
    "  return {\n",
    "    \"python\": True,\n",
    "    \"cache_text_encoder_outputs.py\": True if \"hun\" in model_train else False,\n",
    "    \"wan_cache_text_encoder_outputs.py\": True if \"wan\" in model_train else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--text_encoder1\": f'\"{model_path(\"hun_text_encoder1\")}\"' if \"hun\" in model_train else False,\n",
    "    \"--text_encoder2\": f'\"{model_path(\"hun_text_encoder2\")}\"' if \"hun\" in model_train else False,\n",
    "    \"--t5\": f'\"{model_path(\"tt5_clip\")}\"' if \"wan\" in model_train else False,\n",
    "    \"--batch_size\": 16\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
