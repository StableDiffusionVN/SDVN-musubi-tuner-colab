{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wan\n",
    "model_train_list = {\n",
    "  \"clip_l\": \"https://huggingface.co/StableDiffusionVN/Flux/blob/main/Clip/clip_l.safetensors\",\n",
    "  \"t5_xxl\": \"https://huggingface.co/StableDiffusionVN/Flux/blob/main/Clip/t5xxl_fp16.safetensors\",\n",
    "  \"flux_vae\": \"https://huggingface.co/StableDiffusionVN/Flux/blob/main/Vae/flux_vae.safetensors\",\n",
    "\n",
    "  \"kontext_dev\": \"https://huggingface.co/StableDiffusionVN/Flux/blob/main/Unet/flux1-kontext-dev.safetensors\",\n",
    "  \"qwen_image_bf16\": \"https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/blob/main/split_files/diffusion_models/qwen_image_bf16.safetensors\",\n",
    "  \"qwen_image_edit_bf16\": \"https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI/blob/main/split_files/diffusion_models/qwen_image_edit_bf16.safetensors\",\n",
    "  \"qwen_image_edit_2509_bf16\": \"https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI/blob/main/split_files/diffusion_models/qwen_image_edit_2509_bf16.safetensors\",\n",
    "  \"qwen_image_edit_2511_bf16\": \"https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI/blob/main/split_files/diffusion_models/qwen_image_edit_2511_bf16.safetensors\",\n",
    "  \"clip_qwen\": \"https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/blob/main/split_files/text_encoders/qwen_2.5_vl_7b.safetensors\",\n",
    "  \"vae_qwen\": \"https://huggingface.co/StableDiffusionVN/QwenImage/blob/main/vae/qwen_vae.safetensors\",\n",
    "\n",
    "  \"z_image_turbo_bf16\": \"https://huggingface.co/Comfy-Org/z_image_turbo/blob/main/split_files/diffusion_models/z_image_turbo_bf16.safetensors\",\n",
    "  \"z_image_de_turbo_v1_bf16\": \"https://huggingface.co/ostris/Z-Image-De-Turbo/blob/main/z_image_de_turbo_v1_bf16.safetensors\",\n",
    "  \"qwen_3_4b\": \"https://huggingface.co/Comfy-Org/z_image_turbo/blob/main/split_files/text_encoders/qwen_3_4b.safetensors\",\n",
    "  \"vae_z\": \"https://huggingface.co/Comfy-Org/z_image_turbo/blob/main/split_files/vae/ae.safetensors\",\n",
    "  \"z_adapter_v2\": \"https://huggingface.co/ostris/zimage_turbo_training_adapter/blob/main/zimage_turbo_training_adapter_v2.safetensors\",\n",
    "  \n",
    "  \"wan21_t2v_14B\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_t2v_14B_fp8_e4m3fn.safetensors\",\n",
    "  \"wan21_i2v_14B\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp8_e4m3fn.safetensors\",\n",
    "\n",
    "  \"vae_wan21\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/vae/wan_2.1_vae.safetensors\",\n",
    "  \"umt5_clip\": \"https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/blob/main/models_t5_umt5-xxl-enc-bf16.pth\",\n",
    "  \"wan_clip_vision\": \"https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth\",\n",
    "\n",
    "  \"wan22_t2v_low_14B\": \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp16.safetensors\",\n",
    "  \"wan22_t2v_high_14B\": \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp16.safetensors\",\n",
    "\n",
    "  \"wan22_i2v_low_14B\": \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/wan2.2_i2v_low_noise_14B_fp16.safetensors\",\n",
    "  \"wan22_i2v_high_14B\": \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/wan2.2_i2v_high_noise_14B_fp16.safetensors\",\n",
    "}\n",
    "\n",
    "#model_dic\n",
    "\n",
    "model_dic = {\n",
    "    \"kontext_dev\": {\n",
    "        \"model\": \"kontext_dev\",\n",
    "        \"vae\": \"flux_vae\",\n",
    "        \"clip\": \"t5_xxl\",\n",
    "        \"clip2\": \"clip_l\",\n",
    "    },\n",
    "    \"qwen_image\": {\n",
    "        \"model\": \"qwen_image_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"qwen_image_edit\": {\n",
    "        \"model\": \"qwen_image_edit_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"qwen_image_edit_2509\": {\n",
    "        \"model\": \"qwen_image_edit_2509_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"qwen_image_edit_2511\": {\n",
    "        \"model\": \"qwen_image_edit_2511_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"z_image_turbo\": {\n",
    "        \"model\": \"z_image_de_turbo_v1_bf16\",\n",
    "        \"vae\": \"vae_z\",\n",
    "        \"clip\": \"qwen_3_4b\",\n",
    "        \"adaptar\": \"z_adapter_v2\"\n",
    "    },\n",
    "    \"finetuning-z_image_turbo\": {\n",
    "        \"model\": \"z_image_de_turbo_v1_bf16\",\n",
    "        \"vae\": \"vae_z\",\n",
    "        \"clip\": \"qwen_3_4b\",\n",
    "    },\n",
    "    \"finetuning-qwen_image\": {\n",
    "        \"model\": \"qwen_image_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"finetuning-qwen_image_edit\": {\n",
    "        \"model\": \"qwen_image_edit_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"finetuning-qwen_image_edit_2509\": {\n",
    "        \"model\": \"qwen_image_edit_2509_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"finetuning-qwen_image_edit_2511\": {\n",
    "        \"model\": \"qwen_image_edit_2511_bf16\",\n",
    "        \"vae\": \"vae_qwen\",\n",
    "        \"clip\": \"clip_qwen\",\n",
    "    },\n",
    "    \"wan21_t2v_14B\": {\n",
    "        \"model\": \"wan21_t2v_14B\",\n",
    "        \"vae\": \"vae_wan21\",\n",
    "        \"clip\": \"umt5_clip\",\n",
    "        \"task\": \"t2v-14B\",\n",
    "        \"discrete_flow_shift\": 3,\n",
    "    },\n",
    "    \"wan21_i2v_14B\": {\n",
    "        \"model\": \"wan21_i2v_14B\",\n",
    "        \"vae\": \"vae_wan21\",\n",
    "        \"clip\": \"umt5_clip\",\n",
    "        \"clip_vision\":\"wan_clip_vision\",\n",
    "        \"task\": \"i2v-14B\",\n",
    "        \"discrete_flow_shift\": 3,\n",
    "    },\n",
    "    \"wan22_t2v_14B\": {\n",
    "        \"model\": \"wan22_t2v_low_14B\",\n",
    "        \"model2\": \"wan22_t2v_high_14B\",\n",
    "        \"vae\": \"vae_wan21\",\n",
    "        \"clip\": \"umt5_clip\",\n",
    "        \"task\": \"t2v-A14B\",\n",
    "        \"discrete_flow_shift\": 12,\n",
    "    },\n",
    "    \"wan22_i2v_14B\": {\n",
    "        \"model\": \"wan22_i2v_low_14B\",\n",
    "        \"model2\": \"wan22_i2v_high_14B\",\n",
    "        \"vae\": \"vae_wan21\",\n",
    "        \"clip\": \"umt5_clip\",\n",
    "        \"task\": \"i2v-A14B\",\n",
    "        \"discrete_flow_shift\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_folder = os.path.join(root_dir,\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8KQYvGDO7cg"
   },
   "outputs": [],
   "source": [
    "def dic2arg(config:dict):\n",
    "  arg = ''\n",
    "  for value in config:\n",
    "    arg += f'{value if str(config[value]) != \"False\" else \"\"} {\"\" if type(config[value]) == bool else config[value]} '\n",
    "  return arg\n",
    "\n",
    "def hug_down(link,path):\n",
    "  name = path.split('/')[-1]\n",
    "  folder = path.split(name)[0]\n",
    "  if \"blob\" in link:\n",
    "    link = link.replace(\"blob\",\"resolve\")\n",
    "  !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -s 16 -k 1M {link} -d {folder} -o {name}\n",
    "\n",
    "def model_path(model):\n",
    "  model_name = model\n",
    "  model_name = model_train_list[model].split('/')[-1]\n",
    "  model_path = os.path.join(model_folder,model_name)\n",
    "  if not os.path.exists(model_path):\n",
    "    hug_down(model_train_list[model],model_path)\n",
    "  return model_path\n",
    "\n",
    "op = {\n",
    "    '--mixed_precision': 'bf16',\n",
    "    '--num_cpu_threads_per_process': 1,\n",
    "}\n",
    "\n",
    "qwen_version = {\n",
    "    \"qwen_image\": \"original\",\n",
    "    \"qwen_image_edit\": \"edit\",\n",
    "    \"qwen_image_edit_2509\": \"edit-2509\",\n",
    "    \"qwen_image_edit_2511\": \"edit-2511\"\n",
    "}\n",
    "\n",
    "if \"kontext\" in model_type:\n",
    "  timestep_sampling = \"flux_shift\"\n",
    "  networks = \"networks.lora_flux\"\n",
    "elif \"qwen\" in model_type:\n",
    "  discrete_flow_shift = 2.2\n",
    "  timestep_sampling = \"shift\"\n",
    "  networks = \"networks.lora_qwen_image\"\n",
    "  \n",
    "elif \"z_image\" in model_type:\n",
    "  discrete_flow_shift = 2.0\n",
    "  timestep_sampling = \"shift\"\n",
    "  networks = \"networks.lora_zimage\"\n",
    "\n",
    "def fun_data_config():\n",
    "  return {\n",
    "    \"general\": {\n",
    "        \"resolution\": resolution,\n",
    "        \"caption_extension\": extension,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"enable_bucket\": True,\n",
    "        \"bucket_no_upscale\": False\n",
    "    },\n",
    "    \"datasets\": []\n",
    "  }\n",
    "\n",
    "def fun_config():\n",
    "  dataset_config_path = os.path.join(root_dir,\"dataset.toml\")\n",
    "  return {\n",
    "    \"flux_kontext_train_network.py\": True if \"kontext\" in model_type else False,\n",
    "    \"qwen_image_train_network.py\": True if \"qwen\" in model_type else False,\n",
    "    \"zimage_train_network.py\": True if \"z_image\" in model_type else False,\n",
    "    \"--dit\": f'\"{model_path(model_dic[model_type][\"model\"])}\"' if model_train == \"\" else download_lib(model_train),\n",
    "    \"--vae\": f'\"{model_path(model_dic[model_type][\"vae\"])}\"',\n",
    "    \"--text_encoder\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"clip2\" not in model_dic[model_type] else False,\n",
    "    \"--text_encoder1\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"clip2\" in model_dic[model_type] else False,\n",
    "    \"--text_encoder2\": f'\"{model_path(model_dic[model_type][\"clip2\"])}\"' if \"clip2\" in model_dic[model_type] else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--model_version\": qwen_version[model_type] if \"qwen\" in model_type else False,\n",
    "    \"--sdpa\": True,\n",
    "    \"--flash_attn\": False,\n",
    "    \"--flash3\": False,\n",
    "    \"--sage_attn\": False,\n",
    "    \"--xformers\": False,\n",
    "    \"--split_attn\": True,\n",
    "    \"--mixed_precision\": \"bf16\",\n",
    "    \"--fp8_base\": fp8_base,\n",
    "    \"--fp8_vl\": fp8_vl if \"qwen\" in model_type else False,\n",
    "    \"--optimizer_type\": optimizer_type,\n",
    "    \"--optimizer_args\": False,\n",
    "    \"--learning_rate\": learning_rate,\n",
    "    \"--lr_scheduler\": lr_scheduler,\n",
    "    \"--lr_warmup_steps\": lr_warmup_steps if lr_scheduler == \"constant_with_warmup\" else False,\n",
    "    \"--lr_scheduler_power\": lr_poly_power if lr_scheduler == \"polynomial\" else False,\n",
    "    \"--lr_scheduler_num_cycles\": lr_restarts_num_cycles if lr_scheduler == \"cosine_with_restarts\" else False,\n",
    "    \"--gradient_checkpointing\": gradient_checkpointing,\n",
    "    \"--gradient_accumulation_steps\": False,\n",
    "    \"--max_data_loader_n_workers\": 2,\n",
    "    \"--persistent_data_loader_workers\": True,\n",
    "    \"--network_module\": networks,\n",
    "    \"--network_dim\": network_dim,\n",
    "    \"--network_alpha\": network_alpha,\n",
    "    \"--network_dropout\": False,\n",
    "    \"--network_weights\": False,\n",
    "    \"--blocks_to_swap\": 16 if blocks_to_swap and \"qwen\" in model_type else False,\n",
    "    \"--timestep_sampling\": timestep_sampling,\n",
    "    \"--min_timestep\": min_timestep,\n",
    "    \"--max_timestep\": max_timestep,\n",
    "    \"--preserve_distribution_shape\": preserve_distribution_shape,\n",
    "    \"--discrete_flow_shift\": discrete_flow_shift,\n",
    "    \"--weighting_scheme\": \"none\",\n",
    "    \"--max_train_steps\": False if max_train_steps <= 0 else max_train_steps,\n",
    "    \"--max_train_epochs\": False if max_train_epochs <= 0 else max_train_epochs,\n",
    "    \"--save_every_n_epochs\": 1 if save_every_n_epochs < 1 else save_every_n_epochs,\n",
    "    \"--save_last_n_epochs\": False if save_last_n_epochs <= 0 else save_last_n_epochs,\n",
    "    \"--save_every_n_steps\": False if save_every_n_steps <= 0 else save_every_n_steps,\n",
    "    \"--seed\": 42,\n",
    "    \"--output_dir\": f'\"{output_dir}/{output_name}_{model_type}\"',\n",
    "    \"--output_name\": f'\"{output_name}\"'\n",
    "}\n",
    "\n",
    "def fun_config_finetuning():\n",
    "  dataset_config_path = os.path.join(root_dir,\"dataset.toml\")\n",
    "  return {\n",
    "    \"zimage_train.py\": True if \"z_image\" in model_type else False,\n",
    "    \"qwen_image_train.py\": True if \"qwen\" in model_type else False,\n",
    "    \"--dit\": f'\"{model_path(model_dic[model_type][\"model\"])}\"' if model_train == \"\" else download_lib(model_train),\n",
    "    \"--vae\": f'\"{model_path(model_dic[model_type][\"vae\"])}\"',\n",
    "    \"--text_encoder\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"clip2\" not in model_dic[model_type] else False,\n",
    "    \"--text_encoder1\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"clip2\" in model_dic[model_type] else False,\n",
    "    \"--text_encoder2\": f'\"{model_path(model_dic[model_type][\"clip2\"])}\"' if \"clip2\" in model_dic[model_type] else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--model_version\": qwen_version[model_type] if \"qwen\" in model_type else False,\n",
    "    \"--sdpa\": True,\n",
    "    \"--split_attn\": True,\n",
    "    \"--mixed_precision\": \"bf16\",\n",
    "    \"--full_bf16\": full_bf16,\n",
    "    \"--gradient_checkpointing\": gradient_checkpointing,\n",
    "    \"--optimizer_type\": \"adafactor\",\n",
    "    \"--fused_backward_pass\": True,\n",
    "    \"--optimizer_args\": '\"relative_step=False\" \"scale_parameter=False\" \"warmup_init=False\"',\n",
    "    \"--learning_rate\": learning_rate,\n",
    "    \"--lr_scheduler\": lr_scheduler,\n",
    "    \"--lr_warmup_steps\": lr_warmup_steps if lr_scheduler == \"constant_with_warmup\" else False,\n",
    "    \"--lr_scheduler_power\": lr_poly_power if lr_scheduler == \"polynomial\" else False,\n",
    "    \"--lr_scheduler_num_cycles\": lr_restarts_num_cycles if lr_scheduler == \"cosine_with_restarts\" else False,\n",
    "    \"--timestep_sampling\": timestep_sampling,\n",
    "    \"--min_timestep\": min_timestep,\n",
    "    \"--max_timestep\": max_timestep,\n",
    "    \"--preserve_distribution_shape\": preserve_distribution_shape,\n",
    "    \"--max_data_loader_n_workers\": 2,\n",
    "    \"--persistent_data_loader_workers\": True,\n",
    "    \"--max_train_steps\": False if max_train_steps <= 0 else max_train_steps,\n",
    "    \"--max_train_epochs\": False if max_train_epochs <= 0 else max_train_epochs,\n",
    "    \"--save_every_n_epochs\": 1 if save_every_n_epochs < 1 else save_every_n_epochs,\n",
    "    \"--save_last_n_epochs\": False if save_last_n_epochs <= 0 else save_last_n_epochs,\n",
    "    \"--save_every_n_steps\": False if save_every_n_steps <= 0 else save_every_n_steps,\n",
    "    \"--seed\": 42,\n",
    "    \"--output_dir\": f'\"{output_dir.split(\"/Lora\")[0]}/ComfyModel/diffusion_models/{output_name}_{model_type}\"',\n",
    "    \"--output_name\": f'\"{output_name}\"'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dataset_config_path = os.path.join(root_dir,\"dataset.toml\")\n",
    "if \"bf16\" in model_train_list[model_dic[model_type][\"model\"]]:\n",
    "  mixed_precision = \"bf16\"\n",
    "elif \"fp16\" in model_train_list[model_dic[model_type][\"model\"]]:\n",
    "  mixed_precision = \"fp16\"\n",
    "else:\n",
    "  mixed_precision = None\n",
    "\n",
    "def fun_config_video():\n",
    "  dataset_config_path = os.path.join(root_dir,\"dataset.toml\")\n",
    "  return {\n",
    "    \"wan_train_network.py\": True if \"wan\" in model_type else False,\n",
    "    \"--task\": model_dic[model_type][\"task\"] if \"wan\" in model_type else False,\n",
    "    \"--dit\": f'\"{model_path(model_dic[model_type][\"model\"])}\"',\n",
    "    \"--dit_high_noise\": f'\"{model_path(model_dic[model_type][\"model2\"])}\"' if \"model2\" in model_dic[model_type] else False,\n",
    "    \"--vae\": f'\"{model_path(model_dic[model_type][\"vae\"])}\"',\n",
    "    \"--t5\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"wan\" in model_type else False,\n",
    "    \"--clip\": f'\"{model_path(model_dic[model_type][\"clip_vision\"])}\"' if \"wan21_i2v\" in model_type else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--sdpa\": True,\n",
    "    \"--split_attn\": True,\n",
    "    \"--mixed_precision\": mixed_precision,\n",
    "    \"--fp8_base\": fp8_base,\n",
    "    \"--optimizer_type\": optimizer_type,\n",
    "    \"--optimizer_args\": False,\n",
    "    \"--learning_rate\": learning_rate,\n",
    "    \"--lr_scheduler\": lr_scheduler,\n",
    "    \"--lr_warmup_steps\": lr_warmup_steps if lr_scheduler == \"constant_with_warmup\" else False,\n",
    "    \"--lr_scheduler_power\": lr_poly_power if lr_scheduler == \"polynomial\" else False,\n",
    "    \"--lr_scheduler_num_cycles\": lr_restarts_num_cycles if lr_scheduler == \"cosine_with_restarts\" else False,\n",
    "    \"--gradient_checkpointing\": gradient_checkpointing,\n",
    "    \"--gradient_accumulation_steps\": False,\n",
    "    \"--max_data_loader_n_workers\": 2,\n",
    "    \"--persistent_data_loader_workers\": True,\n",
    "    \"--network_module\": \"networks.lora_wan\",\n",
    "    \"--network_dim\": network_dim,\n",
    "    \"--network_alpha\": network_alpha,\n",
    "    \"--network_dropout\": False,\n",
    "    \"--network_weights\": False,\n",
    "    \"--timestep_sampling\": timestep_sampling,\n",
    "    \"--min_timestep\": min_timestep,\n",
    "    \"--max_timestep\": max_timestep,\n",
    "    \"--preserve_distribution_shape\": preserve_distribution_shape,\n",
    "    \"--discrete_flow_shift\": model_dic[model_type][\"discrete_flow_shift\"],\n",
    "    \"--timestep_boundary\": timestep_boundary if \"wan22\" in model_type else False,\n",
    "    \"--max_train_steps\": max_train_steps if max_train_steps > 0 else False,\n",
    "    \"--max_train_epochs\": max_train_epochs if max_train_epochs > 0 else False,\n",
    "    \"--save_every_n_epochs\": save_every_n_epochs if save_every_n_epochs >= 1 else 1,\n",
    "    \"--save_last_n_epochs\": False if save_last_n_epochs <= 0 else save_last_n_epochs,\n",
    "    \"--save_every_n_steps\": False if save_every_n_steps <= 0 else save_every_n_steps,\n",
    "    \"--seed\": 42,\n",
    "    \"--output_dir\": f'\"{output_dir}/{output_name}_{model_type}\"',\n",
    "    \"--output_name\": f'\"{output_name}\"'\n",
    "}\n",
    "\n",
    "sample_prompt_path = os.path.join(root_dir,\"prompt.txt\")\n",
    "\n",
    "def fun_extra():\n",
    "  return {\n",
    "    \"--sample_prompts\": f'\"{sample_prompt_path}\"' if sample_every_n_steps > 0 else False,\n",
    "    \"--sample_every_n_epochs\": False,\n",
    "    \"--sample_every_n_steps\": sample_every_n_steps if sample_every_n_steps > 0 else False,\n",
    "    \"--sample_at_first\": True if sample_every_n_steps > 0 else False,\n",
    "    \"--log_with\": \"wandb\" if wandb_api_key != \"\" else False,\n",
    "    \"--wandb_api_key\": wandb_api_key if wandb_api_key != \"\" else False,\n",
    "    \"--wandb_run_name\": f'\"{model_type}_{output_name}\"',\n",
    "    \"--save_state\": save_state,\n",
    "    \"--save_state_on_train_end\": save_state_on_train_end,\n",
    "    \"--resume\": resume if resume != \"\" else False,\n",
    "    '--metadata_title': f'\"{output_name}\"',\n",
    "    '--metadata_author': f'\"{author}\"',\n",
    "    '--metadata_description': f'\"{description}\"',\n",
    "    '--metadata_license': False, # \"MIT\"\n",
    "    '--metadata_tags': False, # \"sdvn.me\"\n",
    "}\n",
    "\n",
    "def fun_prompt(sample_prompt, sample_image_path):\n",
    "  if \"kontext\" in model_type or \"edit\" in model_type:\n",
    "    qwen_size = True if 'qwen' in model_type else False\n",
    "    if ControlFolder != \"\":\n",
    "      s, p = random_sample(folder_train,ControlFolder)\n",
    "    else:\n",
    "      s, p = random_sample(folder_train,folder_train)\n",
    "    if sample_prompt == \"\" :\n",
    "      sample_prompt = s\n",
    "    if sample_image_path == \"\":\n",
    "      sample_image_path = p\n",
    "    prompt = f\"\"\"{sample_prompt} --w {image_size(sample_image_path, qwensize = qwen_size)[0]} --h {image_size(sample_image_path, qwensize = qwen_size)[1]} --f 25 --ci {sample_image_path}\"\"\"\n",
    "  else:\n",
    "    prompt = f\"\"\"{sample_prompt if sample_prompt != \"\" else random_sample(folder_train)[0]} --w {sample_size[0]} --h {sample_size[1]} --f 25\"\"\"\n",
    "\n",
    "  write_file(sample_prompt_path,prompt)\n",
    "\n",
    "def fun_prompt_video(sample_prompt, sample_image_path):\n",
    "  if \"i2v\" in model_type:\n",
    "    prompt = f\"\"\"{sample_prompt if sample_prompt != \"\" else random_sample(folder_train)} --w {sample_size[0]} --h {sample_size[1]} --f 25 --i {sample_image_path}\"\"\"\n",
    "  else:\n",
    "    prompt = f\"\"\"{sample_prompt if sample_prompt != \"\" else random_sample(folder_train)} --w {sample_size[0]} --h {sample_size[1]} --f 25\"\"\"\n",
    "\n",
    "  write_file(sample_prompt_path,prompt)\n",
    "\n",
    "\n",
    "def fun_dataset_file():\n",
    "  dataset = os.path.join(root_dir,\"dataset.toml\")\n",
    "  for dir in check_sub_dir(image_dir):\n",
    "    data_config[\"datasets\"].append({\n",
    "        \"image_directory\": dir,\n",
    "        \"control_directory\": dir.replace(ImageFolder, ControlFolder) if ControlFolder != \"\" else None,\n",
    "        \"cache_directory\": os.path.join(root_dir,f\"cache_{model_type}{dir.split(image_dir)[-1]}\"),\n",
    "        \"num_repeats\": repeat_dir(dir,num_repeats),\n",
    "        \"flux_kontext_no_resize_control\": True if resize_control == False and \"kontext\" in model_type else None,\n",
    "        })\n",
    "\n",
    "  with open(dataset, \"w\") as file:\n",
    "      toml.dump(data_config, file)\n",
    "\n",
    "def fun_dataset_file_video():\n",
    "  dataset = os.path.join(root_dir,\"dataset.toml\")\n",
    "  if Image_train:\n",
    "    for dir in check_sub_dir(image_dir):\n",
    "      data_config[\"datasets\"].append({\n",
    "            \"image_directory\": dir,\n",
    "            \"cache_directory\": os.path.join(root_dir,f\"cache_{model_type}\"),\n",
    "            \"num_repeats\": repeat_dir(dir,num_repeats),\n",
    "            })\n",
    "  if Video_train:\n",
    "    for dir in check_sub_dir(video_dir):\n",
    "      data_config[\"datasets\"].append({\n",
    "            \"video_directory\": dir,\n",
    "            \"cache_directory\": os.path.join(root_dir,f\"cachevideo_{model_type}\"),\n",
    "            \"num_repeats\": repeat_dir(dir,num_repeats),\n",
    "            \"frame_extraction\": frame_extraction,\n",
    "            \"target_frames\": target_frames,\n",
    "            \"frame_stride\": frame_stride,\n",
    "            \"frame_sample\": frame_sample,\n",
    "            \"max_frames\": max_frames,\n",
    "        })\n",
    "\n",
    "  with open(dataset, \"w\") as file:\n",
    "      toml.dump(data_config, file)\n",
    "\n",
    "dataset_config_path = os.path.join(root_dir,\"dataset.toml\")\n",
    "\n",
    "def fun_cache_latents_config():\n",
    "  return {\n",
    "    \"python\": True,\n",
    "    \"flux_kontext_cache_latents.py\": True if \"kontext\" in model_type else False,\n",
    "    \"src/musubi_tuner/qwen_image_cache_latents.py\": True if \"qwen\" in model_type else False,\n",
    "    \"src/musubi_tuner/zimage_cache_latents.py\": True if \"z_image\" in model_type else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--vae\": f'\"{model_path(model_dic[model_type][\"vae\"])}\"',\n",
    "    \"--model_version\": qwen_version[model_type] if \"qwen\" in model_type else False,\n",
    "}\n",
    "\n",
    "def fun_cache_latents_config_video():\n",
    "  return {\n",
    "    \"python\": True,\n",
    "    \"wan_cache_latents.py\": True if \"wan\" in model_type else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--vae\": f'\"{model_path(model_dic[model_type][\"vae\"])}\"',\n",
    "    \"--i2v\": True if \"i2v\" in model_type else False,\n",
    "    \"--clip\": f'\"{model_path(model_dic[model_type][\"clip_vision\"])}\"' if \"wan21_i2v\" in model_type else False,\n",
    "}\n",
    "\n",
    "\n",
    "def fun_cache_text_encoder_config():\n",
    "  return {\n",
    "    \"python\": True,\n",
    "    \"flux_kontext_cache_text_encoder_outputs.py\": True if \"kontext\" in model_type else False,\n",
    "    \"src/musubi_tuner/qwen_image_cache_text_encoder_outputs.py\": True if \"qwen\" in model_type else False,\n",
    "    \"src/musubi_tuner/zimage_cache_text_encoder_outputs.py\": True if \"z_image\" in model_type else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--text_encoder\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"clip2\" not in model_dic[model_type] else False,\n",
    "    \"--text_encoder1\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"clip2\" in model_dic[model_type] else False,\n",
    "    \"--text_encoder2\": f'\"{model_path(model_dic[model_type][\"clip2\"])}\"' if \"clip2\" in model_dic[model_type] else False,\n",
    "    \"--batch_size\": 16,\n",
    "    \"--model_version\": qwen_version[model_type] if \"qwen\" in model_type else False,\n",
    "}\n",
    "\n",
    "def fun_cache_text_encoder_config_video():\n",
    "  return {\n",
    "    \"python\": True,\n",
    "    \"wan_cache_text_encoder_outputs.py\": True if \"wan\" in model_type else False,\n",
    "    \"--dataset_config\": f'\"{dataset_config_path}\"',\n",
    "    \"--t5\": f'\"{model_path(model_dic[model_type][\"clip\"])}\"' if \"wan\" in model_type else False,\n",
    "    \"--batch_size\": 16\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
